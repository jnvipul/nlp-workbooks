{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Load in the data <br>\n",
    "2) Build and train the model <br>\n",
    "3) Test the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram distribution <br>\n",
    "Used in negative sampling and dropping out words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is it supposed to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each epoc:\n",
    "    for each sentence:\n",
    "        # remove some words from the sentence according to p_drop\n",
    "        for each middle_word:\n",
    "            # looping through each word in the sentence\n",
    "            # the word is word and rest is context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit as sigmoid\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    return s.translate(str.maketrans('','',string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow_classifier.py            make_text_corpus.py\n",
      "enwiki-latest-abstract-1.txt pretrained_glove.py\n",
      "enwiki-latest-abstract.xml   word2vec.ipynb\n",
      "finished counting\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], {'<UNK>': 0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls\n",
    "def get_wiki():\n",
    "  V = 20000\n",
    "  files = glob('enwiki-latest-abstract-1.txt')\n",
    "  all_word_counts = {}\n",
    "  for f in files:\n",
    "    for line in open(f):\n",
    "      if line and line[0] not in '[*-|=\\{\\}':\n",
    "        s = remove_punctuation(line).lower().split()\n",
    "        if len(s) > 1:\n",
    "          for word in s:\n",
    "            if word not in all_word_counts:\n",
    "              all_word_counts[word] = 0\n",
    "            all_word_counts[word] += 1\n",
    "  print(\"finished counting\")\n",
    "\n",
    "  V = min(V, len(all_word_counts))\n",
    "  all_word_counts = sorted(all_word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  top_words = [w for w, count in all_word_counts[:V-1]] + ['<UNK>']\n",
    "  word2idx = {w:i for i, w in enumerate(top_words)}\n",
    "  unk = word2idx['<UNK>']\n",
    "\n",
    "  sents = []\n",
    "  for f in files:\n",
    "    for line in open(f):\n",
    "      if line and line[0] not in '[*-|=\\{\\}':\n",
    "        s = remove_punctuation(line).lower().split()\n",
    "        if len(s) > 1:\n",
    "          # if a word is not nearby another word, there won't be any context!\n",
    "          # and hence nothing to train!\n",
    "          sent = [word2idx[w] if w in word2idx else unk for w in s]\n",
    "          sents.append(sent)\n",
    "  print(len(sents))\n",
    "  return sents, word2idx\n",
    "\n",
    "get_wiki()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow_classifier.py            make_text_corpus.py\r\n",
      "enwiki-latest-abstract-1.txt pretrained_glove.py\r\n",
      "enwiki-latest-abstract.xml   word2vec.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "def train_model(savedir):\n",
    "    # get the data\n",
    "    sentences, word2idx = get_wiki()\n",
    "    \n",
    "    # number of unique words\n",
    "    vocab_size = len(word2idx)\n",
    "    \n",
    "    print(f'Vocab size : {vocab_size}')\n",
    "    print(f'sentences : {len(sentences)}')\n",
    "    \n",
    "    # configuration\n",
    "    window_size = 5\n",
    "    learning_rate = 0.025\n",
    "    final_learning_rate = 0.0001\n",
    "    num_negatives = 5\n",
    "    epochs = 20\n",
    "    D = 50\n",
    "    \n",
    "    learning_rate_delta = (learning_rate - final_learning_rate)  \\\n",
    "        / epochs\n",
    "    W = np.random.randn(vocab_size, D) #input-to-hidden\n",
    "    V = np.random.randn(D, vocab_size) #hidden-to-output\n",
    "    \n",
    "    # distribution for drawing negative samples\n",
    "    p_neg = get_negative_sampling_distribution(sentences, vocab_size)\n",
    "        \n",
    "    # save cost to plot later\n",
    "    costs = []\n",
    "    \n",
    "    # total words in corpus (included repeated words)\n",
    "    total_words = sum(len(sentence) for sentence in sentences)\n",
    "    \n",
    "    print(f'total words : {total_words}')\n",
    "    \n",
    "    # subsampling sentence\n",
    "    threshold = 1e-5\n",
    "    p_drop = 1 - np.sqrt(threshold / p_neg)\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        # shuffle sentences so that we don't see them in same order\n",
    "        np.random.shuffle(sentences)\n",
    "        \n",
    "        # accumulate the cost\n",
    "        cost = 0\n",
    "        counter = 0\n",
    "        t0 = datetime.now()\n",
    "        for sentence in sentences:\n",
    "            # keep only certain words based on p_neg\n",
    "            sentence = [w for w in sentence \\\n",
    "                        if np.random.random() < (1-p_drop[w])\n",
    "                       ]\n",
    "            \n",
    "            if len(sentence) < 2:\n",
    "                continue\n",
    "        \n",
    "            # randomly order words so we don't always see\n",
    "            # sample in same order\n",
    "            randomly_ordered_positions = np.random.choice(\n",
    "            len(sentence),\n",
    "            size=len(sentence),#np.random.randint(1, len(sentence) + 1),\n",
    "            replace=False,\n",
    "            )\n",
    "\n",
    "            for pos in randomly_ordered_positions:\n",
    "                # the middle word\n",
    "                word = sentence[pos]\n",
    "\n",
    "                # get the positive context words/negative samples\n",
    "                context_words = get_context(pos, sentence, window_size)\n",
    "                neg_word = np.random.choice(vocab_size, p=p_neg)\n",
    "                targets = np.array(context_words)\n",
    "\n",
    "                c = sgd(word, targets, 1, learning_rate, W, V)\n",
    "                cost += c\n",
    "\n",
    "                c = sgd(neg_word, targets, 0, learning_rate, W, V)\n",
    "                cost += c\n",
    "            \n",
    "            counter += 1\n",
    "            if counter % 100 == 0:\n",
    "                sys.stdout.write(\"processed %s / %s\\r\" % (counter, len(sentences)))\n",
    "                sys.stdout.flush()\n",
    "            \n",
    "        # print stuff so we don't stare at a blank screen\n",
    "        dt = datetime.now() - t0\n",
    "        print(\"epoch complete:\", epoch, \"cost:\", cost, \"dt:\", dt)\n",
    "\n",
    "        # save the cost\n",
    "        costs.append(cost)\n",
    "\n",
    "        # update the learning rate\n",
    "        learning_rate -= learning_rate_delta\n",
    "    \n",
    "    # plot the cost per iteration\n",
    "    plt.plot(costs)\n",
    "    plt.show()\n",
    "    \n",
    "    # save the model\n",
    "    if not os.path.exists(savedir):\n",
    "        os.mkdir(savedir)\n",
    "\n",
    "    with open('%s/word2idx.json' % savedir, 'w') as f:\n",
    "        json.dump(word2idx, f)\n",
    "\n",
    "    np.savez('%s/weights.npz' % savedir, W, V)\n",
    "\n",
    "    # return the model\n",
    "    return word2idx, W, V\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_sampling_distribution(sentences, vocab_size):\n",
    "  # Pn(w) = prob of word occuring\n",
    "  # we would like to sample the negative samples\n",
    "  # such that words that occur more often\n",
    "  # should be sampled more often\n",
    "\n",
    "  word_freq = np.zeros(vocab_size)\n",
    "  word_count = sum(len(sentence) for sentence in sentences)\n",
    "  for sentence in sentences:\n",
    "      for word in sentence:\n",
    "          word_freq[word] += 1\n",
    "\n",
    "  # smooth it\n",
    "  p_neg = word_freq**0.75\n",
    "\n",
    "  # normalize it\n",
    "  p_neg = p_neg / p_neg.sum()\n",
    "\n",
    "  assert(np.all(p_neg > 0))\n",
    "  return p_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished counting\n",
      "Vocab size : 1\n",
      "sentences : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vipul/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-ebf160d3a02c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mword2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'w2v_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m     \u001b[0;31m# word2idx, W, V = load_model('w2v_model')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;31m#     test_model(word2idx, W, V)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-a3fbb32eceda>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(savedir)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# distribution for drawing negative samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mp_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_negative_sampling_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# save cost to plot later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-acf5687bfa6a>\u001b[0m in \u001b[0;36mget_negative_sampling_distribution\u001b[0;34m(sentences, vocab_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mp_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_neg\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mp_neg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_neg\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mp_neg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_context(pos, sentence, window_size):\n",
    "  # input:\n",
    "  # a sentence of the form: x x x x c c c pos c c c x x x x\n",
    "  # output:\n",
    "  # the context word indices: c c c c c c\n",
    "\n",
    "  start = max(0, pos - window_size)\n",
    "  end_  = min(len(sentence), pos + window_size)\n",
    "\n",
    "  context = []\n",
    "  for ctx_pos, ctx_word_idx in enumerate(sentence[start:end_], start=start):\n",
    "    if ctx_pos != pos:\n",
    "      # don't include the input word itself as a target\n",
    "      context.append(ctx_word_idx)\n",
    "  return context\n",
    "\n",
    "\n",
    "def sgd(input_, targets, label, learning_rate, W, V):\n",
    "  # W[input_] shape: D\n",
    "  # V[:,targets] shape: D x N\n",
    "  # activation shape: N\n",
    "  # print(\"input_:\", input_, \"targets:\", targets)\n",
    "  activation = W[input_].dot(V[:,targets])\n",
    "  prob = sigmoid(activation)\n",
    "\n",
    "  # gradients\n",
    "  gV = np.outer(W[input_], prob - label) # D x N\n",
    "  gW = np.sum((prob - label)*V[:,targets], axis=1) # D\n",
    "\n",
    "  V[:,targets] -= learning_rate*gV # D x N\n",
    "  W[input_] -= learning_rate*gW # D\n",
    "\n",
    "  # return cost (binary cross entropy)\n",
    "  cost = label * np.log(prob + 1e-10) + (1 - label) * np.log(1 - prob + 1e-10)\n",
    "  return cost.sum()\n",
    "\n",
    "\n",
    "def load_model(savedir):\n",
    "  with open('%s/word2idx.json' % savedir) as f:\n",
    "    word2idx = json.load(f)\n",
    "  npz = np.load('%s/weights.npz' % savedir)\n",
    "  W = npz['arr_0']\n",
    "  V = npz['arr_1']\n",
    "  return word2idx, W, V\n",
    "\n",
    "\n",
    "\n",
    "def analogy(pos1, neg1, pos2, neg2, word2idx, idx2word, W):\n",
    "    V, D = W.shape\n",
    "\n",
    "    # don't actually use pos2 in calculation, just print what's expected\n",
    "    print(\"testing: %s - %s = %s - %s\" % (pos1, neg1, pos2, neg2))\n",
    "    for w in (pos1, neg1, pos2, neg2):\n",
    "        if w not in word2idx:\n",
    "          print(\"Sorry, %s not in word2idx\" % w)\n",
    "          return\n",
    "\n",
    "    p1 = W[word2idx[pos1]]\n",
    "    n1 = W[word2idx[neg1]]\n",
    "    p2 = W[word2idx[pos2]]\n",
    "    n2 = W[word2idx[neg2]]\n",
    "\n",
    "    vec = p1 - n1 + n2\n",
    "\n",
    "    distances = pairwise_distances(vec.reshape(1, D), W, metric='cosine').reshape(V)\n",
    "    idx = distances.argsort()[:10]\n",
    "\n",
    "    # pick one that's not p1, n1, or n2\n",
    "    best_idx = -1\n",
    "    keep_out = [word2idx[w] for w in (pos1, neg1, neg2)]\n",
    "    # print(\"keep_out:\", keep_out)\n",
    "    for i in idx:\n",
    "        if i not in keep_out:\n",
    "          best_idx = i\n",
    "          break\n",
    "        # print(\"best_idx:\", best_idx)\n",
    "\n",
    "    print(\"got: %s - %s = %s - %s\" % (pos1, neg1, idx2word[best_idx], neg2))\n",
    "    print(\"closest 10:\")\n",
    "    for i in idx:\n",
    "        print(idx2word[i], distances[i])\n",
    "\n",
    "    print(\"dist to %s:\" % pos2, cos_dist(p2, vec))\n",
    "\n",
    "\n",
    "def test_model(word2idx, W, V):\n",
    "  # there are multiple ways to get the \"final\" word embedding\n",
    "  # We = (W + V.T) / 2\n",
    "  # We = W\n",
    "    idx2word = {i:w for w, i in word2idx.items()}\n",
    "\n",
    "    for We in (W, (W + V.T) / 2):\n",
    "        print(\"**********\")\n",
    "\n",
    "        analogy('king', 'man', 'queen', 'woman', word2idx, idx2word, We)\n",
    "        analogy('king', 'prince', 'queen', 'princess', word2idx, idx2word, We)\n",
    "        analogy('miami', 'florida', 'dallas', 'texas', word2idx, idx2word, We)\n",
    "        analogy('einstein', 'scientist', 'picasso', 'painter', word2idx, idx2word, We)\n",
    "        analogy('japan', 'sushi', 'germany', 'bratwurst', word2idx, idx2word, We)\n",
    "        analogy('man', 'woman', 'he', 'she', word2idx, idx2word, We)\n",
    "        analogy('man', 'woman', 'uncle', 'aunt', word2idx, idx2word, We)\n",
    "        analogy('man', 'woman', 'brother', 'sister', word2idx, idx2word, We)\n",
    "        analogy('man', 'woman', 'husband', 'wife', word2idx, idx2word, We)\n",
    "        analogy('man', 'woman', 'actor', 'actress', word2idx, idx2word, We)\n",
    "        analogy('man', 'woman', 'father', 'mother', word2idx, idx2word, We)\n",
    "        analogy('heir', 'heiress', 'prince', 'princess', word2idx, idx2word, We)\n",
    "        analogy('nephew', 'niece', 'uncle', 'aunt', word2idx, idx2word, We)\n",
    "        analogy('france', 'paris', 'japan', 'tokyo', word2idx, idx2word, We)\n",
    "        analogy('france', 'paris', 'china', 'beijing', word2idx, idx2word, We)\n",
    "        analogy('february', 'january', 'december', 'november', word2idx, idx2word, We)\n",
    "        analogy('france', 'paris', 'germany', 'berlin', word2idx, idx2word, We)\n",
    "        analogy('week', 'day', 'year', 'month', word2idx, idx2word, We)\n",
    "        analogy('week', 'day', 'hour', 'minute', word2idx, idx2word, We)\n",
    "        analogy('france', 'paris', 'italy', 'rome', word2idx, idx2word, We)\n",
    "        analogy('paris', 'france', 'rome', 'italy', word2idx, idx2word, We)\n",
    "        analogy('france', 'french', 'england', 'english', word2idx, idx2word, We)\n",
    "        analogy('japan', 'japanese', 'china', 'chinese', word2idx, idx2word, We)\n",
    "        analogy('china', 'chinese', 'america', 'american', word2idx, idx2word, We)\n",
    "        analogy('japan', 'japanese', 'italy', 'italian', word2idx, idx2word, We)\n",
    "        analogy('japan', 'japanese', 'australia', 'australian', word2idx, idx2word, We)\n",
    "        analogy('walk', 'walking', 'swim', 'swimming', word2idx, idx2word, We)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    word2idx, W, V = train_model('w2v_model')\n",
    "    # word2idx, W, V = load_model('w2v_model')\n",
    "#     test_model(word2idx, W, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
