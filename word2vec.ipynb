{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Load in the data <br>\n",
    "2) Build and train the model <br>\n",
    "3) Test the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram distribution <br>\n",
    "Used in negative sampling and dropping out words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is it supposed to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each epoc:\n",
    "    for each sentence:\n",
    "        # remove some words from the sentence according to p_drop\n",
    "        for each middle_word:\n",
    "            # looping through each word in the sentence\n",
    "            # the word is word and rest is context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit as sigmoid\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "from glob import glob\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.spatial.distance import cosine as cos_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    return s.translate(str.maketrans('','',string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki():\n",
    "  V = 20000\n",
    "  files = glob('enwiki*.txt')\n",
    "  all_word_counts = {}\n",
    "  for f in files[:5]:\n",
    "    for line in open(f):\n",
    "      if line and line[0] not in '[*-|=\\{\\}':\n",
    "        s = remove_punctuation(line).lower().split()\n",
    "        if len(s) > 1:\n",
    "          for word in s:\n",
    "            if word not in all_word_counts:\n",
    "              all_word_counts[word] = 0\n",
    "            all_word_counts[word] += 1\n",
    "  print(\"finished counting\")\n",
    "\n",
    "  V = min(V, len(all_word_counts))\n",
    "  all_word_counts = sorted(all_word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  top_words = [w for w, count in all_word_counts[:V-1]] + ['<UNK>']\n",
    "  word2idx = {w:i for i, w in enumerate(top_words)}\n",
    "  unk = word2idx['<UNK>']\n",
    "\n",
    "  sents = []\n",
    "  for f in files[:5]:\n",
    "    for line in open(f):\n",
    "      if line and line[0] not in '[*-|=\\{\\}':\n",
    "        s = remove_punctuation(line).lower().split()\n",
    "        if len(s) > 1:\n",
    "          # if a word is not nearby another word, there won't be any context!\n",
    "          # and hence nothing to train!\n",
    "          sent = [word2idx[w] if w in word2idx else unk for w in s]\n",
    "          sents.append(sent)\n",
    "\n",
    "  return sents, word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(savedir):\n",
    "    # get the data\n",
    "    sentences, word2idx = get_wiki()\n",
    "    \n",
    "    # number of unique words\n",
    "    vocab_size = len(word2idx)\n",
    "    \n",
    "    print(f'Vocab size : {vocab_size}')\n",
    "    print(f'sentences : {len(sentences)}')\n",
    "    \n",
    "    # configuration\n",
    "    window_size = 5\n",
    "    learning_rate = 0.025\n",
    "    final_learning_rate = 0.0001\n",
    "    num_negatives = 5\n",
    "    epochs = 23\n",
    "    D = 50\n",
    "    \n",
    "    learning_rate_delta = (learning_rate - final_learning_rate)  \\\n",
    "        / epochs\n",
    "    W = np.random.randn(vocab_size, D) #input-to-hidden\n",
    "    V = np.random.randn(D, vocab_size) #hidden-to-output\n",
    "    \n",
    "    # distribution for drawing negative samples\n",
    "    p_neg = get_negative_sampling_distribution(sentences, vocab_size)\n",
    "        \n",
    "    # save cost to plot later\n",
    "    costs = []\n",
    "    \n",
    "    # total words in corpus (included repeated words)\n",
    "    total_words = sum(len(sentence) for sentence in sentences)\n",
    "    \n",
    "    print(f'total words : {total_words}')\n",
    "    \n",
    "    # subsampling sentence\n",
    "    threshold = 1e-5\n",
    "    p_drop = 1 - np.sqrt(threshold / p_neg)\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        # shuffle sentences so that we don't see them in same order\n",
    "        np.random.shuffle(sentences)\n",
    "        \n",
    "        # accumulate the cost\n",
    "        cost = 0\n",
    "        counter = 0\n",
    "        t0 = datetime.now()\n",
    "        for sentence in sentences:\n",
    "            # keep only certain words based on p_neg\n",
    "            sentence = [w for w in sentence \\\n",
    "                        if np.random.random() < (1-p_drop[w])\n",
    "                       ]\n",
    "            \n",
    "            if len(sentence) < 2:\n",
    "                continue\n",
    "        \n",
    "            # randomly order words so we don't always see\n",
    "            # sample in same order\n",
    "            randomly_ordered_positions = np.random.choice(\n",
    "            len(sentence),\n",
    "            size=len(sentence),#np.random.randint(1, len(sentence) + 1),\n",
    "            replace=False,\n",
    "            )\n",
    "\n",
    "            for pos in randomly_ordered_positions:\n",
    "                # the middle word\n",
    "                word = sentence[pos]\n",
    "\n",
    "                # get the positive context words/negative samples\n",
    "                context_words = get_context(pos, sentence, window_size)\n",
    "                neg_word = np.random.choice(vocab_size, p=p_neg)\n",
    "                targets = np.array(context_words)\n",
    "\n",
    "                c = sgd(word, targets, 1, learning_rate, W, V)\n",
    "                cost += c\n",
    "\n",
    "                c = sgd(neg_word, targets, 0, learning_rate, W, V)\n",
    "                cost += c\n",
    "            \n",
    "            counter += 1\n",
    "            if counter % 100 == 0:\n",
    "                sys.stdout.write(\"processed %s / %s\\r\" % (counter, len(sentences)))\n",
    "                sys.stdout.flush()\n",
    "            \n",
    "        # print stuff so we don't stare at a blank screen\n",
    "        dt = datetime.now() - t0\n",
    "        print(\"epoch complete:\", epoch, \"cost:\", cost, \"dt:\", dt)\n",
    "\n",
    "        # save the cost\n",
    "        costs.append(cost)\n",
    "\n",
    "        # update the learning rate\n",
    "        learning_rate -= learning_rate_delta\n",
    "    \n",
    "    # plot the cost per iteration\n",
    "    plt.plot(costs)\n",
    "    plt.show()\n",
    "    \n",
    "    # save the model\n",
    "    if not os.path.exists(savedir):\n",
    "        os.mkdir(savedir)\n",
    "\n",
    "    with open('%s/word2idx.json' % savedir, 'w') as f:\n",
    "        json.dump(word2idx, f)\n",
    "\n",
    "    np.savez('%s/weights.npz' % savedir, W, V)\n",
    "\n",
    "    # return the model\n",
    "    return word2idx, W, V\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_sampling_distribution(sentences, vocab_size):\n",
    "  # Pn(w) = prob of word occuring\n",
    "  # we would like to sample the negative samples\n",
    "  # such that words that occur more often\n",
    "  # should be sampled more often\n",
    "\n",
    "  word_freq = np.zeros(vocab_size)\n",
    "  word_count = sum(len(sentence) for sentence in sentences)\n",
    "  for sentence in sentences:\n",
    "      for word in sentence:\n",
    "          word_freq[word] += 1\n",
    "\n",
    "  # smooth it\n",
    "  p_neg = word_freq**0.75\n",
    "\n",
    "  # normalize it\n",
    "  p_neg = p_neg / p_neg.sum()\n",
    "\n",
    "  assert(np.all(p_neg > 0))\n",
    "  return p_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "testing: king - man = queen - woman\n",
      "got: king - man = throne - woman\n",
      "closest 10:\n",
      "throne 0.12516467097610495\n",
      "king 0.13197580946748466\n",
      "heir 0.13552692922285026\n",
      "sons 0.15837253860519573\n",
      "grandson 0.18812368955426118\n",
      "queen 0.19253886547028753\n",
      "son 0.1981296144069029\n",
      "duke 0.20034992465471335\n",
      "eleanor 0.20099626507998747\n",
      "emperor 0.2031346550247184\n",
      "dist to queen: 0.19253886547028742\n",
      "testing: king - prince = queen - princess\n",
      "got: king - prince = uncle - princess\n",
      "closest 10:\n",
      "princess 0.07856702742343391\n",
      "king 0.15691851916813637\n",
      "uncle 0.22527054046842732\n",
      "legend 0.2462587183750682\n",
      "son 0.2576401199685705\n",
      "her 0.2659894562396876\n",
      "throne 0.28520572891864016\n",
      "catherine 0.2855645515794554\n",
      "feast 0.28810802919634904\n",
      "grandson 0.2896071490390043\n",
      "dist to queen: 0.3366796961829688\n",
      "testing: miami - florida = dallas - texas\n",
      "got: miami - florida = detroit - texas\n",
      "closest 10:\n",
      "miami 0.13982725427585452\n",
      "texas 0.3795432536270097\n",
      "detroit 0.39109032924524967\n",
      "longtime 0.40266488853497606\n",
      "indians 0.4072094826228141\n",
      "tom 0.4226309766328\n",
      "joe 0.4230629575531917\n",
      "harry 0.4258573923355361\n",
      "seattle 0.42748770041429895\n",
      "tucker 0.4292956041598729\n",
      "dist to dallas: 0.6713238915433923\n",
      "testing: einstein - scientist = picasso - painter\n",
      "got: einstein - scientist = manet - painter\n",
      "closest 10:\n",
      "painter 0.20483671035179052\n",
      "einstein 0.23925240925799662\n",
      "manet 0.30245096411766736\n",
      "gauss 0.31832980138535705\n",
      "feynman 0.3277061386437483\n",
      "painting 0.33248547813166207\n",
      "friedrich 0.3325940074790228\n",
      "claude 0.3428749789336284\n",
      "giovanni 0.34364556585842587\n",
      "leibniz 0.3465647423944114\n",
      "dist to picasso: 0.6109606992224044\n",
      "testing: japan - sushi = germany - bratwurst\n",
      "Sorry, sushi not in word2idx\n",
      "testing: man - woman = he - she\n",
      "got: man - woman = him - she\n",
      "closest 10:\n",
      "him 0.14415086496970075\n",
      "she 0.15796437058560875\n",
      "he 0.1602147056704517\n",
      "i 0.17066704963305712\n",
      "her 0.17803867023877473\n",
      "his 0.18638043687694272\n",
      "man 0.1875223085485378\n",
      "my 0.19038505282474627\n",
      "biographer 0.20957089795910377\n",
      "wrote 0.211930258226393\n",
      "dist to he: 0.16021470567045148\n",
      "testing: man - woman = uncle - aunt\n",
      "got: man - woman = biographer - aunt\n",
      "closest 10:\n",
      "aunt 0.09164141830160921\n",
      "biographer 0.2415771272557209\n",
      "orwell 0.2573288254148691\n",
      "him 0.2658969229708974\n",
      "friends 0.2715087094229125\n",
      "friend 0.2721510830073737\n",
      "brother 0.283679139141351\n",
      "orwells 0.28871628943096117\n",
      "father 0.29484230498104025\n",
      "nephew 0.2954534591389969\n",
      "dist to uncle: 0.3311220359650868\n",
      "testing: man - woman = brother - sister\n",
      "got: man - woman = brother - sister\n",
      "closest 10:\n",
      "sister 0.1791123128062645\n",
      "brother 0.27039994285825286\n",
      "daughter 0.276281928803346\n",
      "father 0.28471121118496057\n",
      "nephew 0.2895698727314755\n",
      "son 0.2985156468163417\n",
      "friend 0.3065577012300209\n",
      "uncle 0.31930040193936116\n",
      "he 0.3221795697016686\n",
      "elder 0.33190517520393104\n",
      "dist to brother: 0.27039994285825286\n",
      "testing: man - woman = husband - wife\n",
      "got: man - woman = him - wife\n",
      "closest 10:\n",
      "wife 0.13601483296312256\n",
      "him 0.14024855596267916\n",
      "son 0.14333321474115346\n",
      "father 0.1623564270577006\n",
      "he 0.16433395445896182\n",
      "brother 0.16526956481282296\n",
      "nephew 0.18254502124955707\n",
      "widow 0.18421120990366102\n",
      "his 0.18777428933278228\n",
      "himself 0.19227488983395302\n",
      "dist to husband: 0.35627844768740746\n",
      "testing: man - woman = actor - actress\n",
      "got: man - woman = starring - actress\n",
      "closest 10:\n",
      "actress 0.09559346580162231\n",
      "starring 0.15166901970221425\n",
      "actor 0.17411970721264447\n",
      "starred 0.18984798966504723\n",
      "dave 0.1989971150729436\n",
      "ian 0.22603086453822052\n",
      "tom 0.23050218013449308\n",
      "jack 0.23090918602808086\n",
      "tim 0.2410888816257457\n",
      "jim 0.24475873466441067\n",
      "dist to actor: 0.1741197072126448\n",
      "testing: man - woman = father - mother\n",
      "got: man - woman = father - mother\n",
      "closest 10:\n",
      "father 0.16966306784648066\n",
      "mother 0.1768763024440494\n",
      "son 0.19929317116270562\n",
      "brother 0.21037071595223378\n",
      "his 0.2215112055826881\n",
      "him 0.22178243212262394\n",
      "he 0.2435138157310116\n",
      "wife 0.24544832370318037\n",
      "himself 0.24632066400372732\n",
      "man 0.2531804043620498\n",
      "dist to father: 0.16966306784648066\n",
      "testing: heir - heiress = prince - princess\n",
      "Sorry, heiress not in word2idx\n",
      "testing: nephew - niece = uncle - aunt\n",
      "got: nephew - niece = soon - aunt\n",
      "closest 10:\n",
      "nephew 0.22406514081902495\n",
      "soon 0.2701004298607239\n",
      "brother 0.2747614067274259\n",
      "aunt 0.2752163818347103\n",
      "son 0.291382409959126\n",
      "wife 0.2933460635830951\n",
      "anna 0.30161235642192086\n",
      "marie 0.30166886916873514\n",
      "widow 0.3091583469019089\n",
      "cousin 0.310876137835691\n",
      "dist to uncle: 0.39216733104072987\n",
      "testing: france - paris = japan - tokyo\n",
      "got: france - paris = japan - tokyo\n",
      "closest 10:\n",
      "tokyo 0.20382831267179058\n",
      "japan 0.2430873579831454\n",
      "japanese 0.31839071916888206\n",
      "tanaka 0.36827162991481166\n",
      "australia 0.3700478820020142\n",
      "zealand 0.3733958001086234\n",
      "launch 0.3739262002756788\n",
      "vietnamese 0.3782612812808671\n",
      "korean 0.3898827286942709\n",
      "vietnam 0.3923396281327107\n",
      "dist to japan: 0.2430873579831454\n",
      "testing: france - paris = china - beijing\n",
      "got: france - paris = korea - beijing\n",
      "closest 10:\n",
      "beijing 0.11777346208959028\n",
      "korea 0.26383188091664755\n",
      "china 0.2823460812446277\n",
      "vietnamese 0.2829445427731241\n",
      "republics 0.3000312377885811\n",
      "taiwan 0.3014476706732728\n",
      "pakistan 0.3057691163819154\n",
      "mainland 0.31440702180346713\n",
      "turkish 0.3243676732157532\n",
      "territories 0.3262507539489704\n",
      "dist to china: 0.2823460812446279\n",
      "testing: february - january = december - november\n",
      "got: february - january = april - november\n",
      "closest 10:\n",
      "november 0.04399973258797896\n",
      "february 0.05113089776883617\n",
      "april 0.06862365471852705\n",
      "october 0.07522762239382796\n",
      "june 0.0789420380120518\n",
      "march 0.0789886072573528\n",
      "july 0.08594373579626824\n",
      "september 0.09053987162672483\n",
      "december 0.09987674375278632\n",
      "august 0.10395481421056063\n",
      "dist to december: 0.09987674375278632\n",
      "testing: france - paris = germany - berlin\n",
      "got: france - paris = germany - berlin\n",
      "closest 10:\n",
      "germany 0.15783958026810552\n",
      "russia 0.18941000614425718\n",
      "germanys 0.19136735691769668\n",
      "poland 0.19260418873840524\n",
      "bloc 0.20721108719593462\n",
      "berlin 0.21306366922235986\n",
      "austria 0.21550100734214683\n",
      "hungary 0.23706848210377252\n",
      "france 0.2410739203636304\n",
      "union 0.24185867913446146\n",
      "dist to germany: 0.15783958026810563\n",
      "testing: week - day = year - month\n",
      "got: week - day = hour - month\n",
      "closest 10:\n",
      "week 0.1416121403745687\n",
      "month 0.17553679372807962\n",
      "hour 0.33373049298689916\n",
      "scheduled 0.33716193981642206\n",
      "14 0.3371711633248131\n",
      "weeks 0.3400876506397883\n",
      "months 0.34044100346645245\n",
      "days 0.350815748490814\n",
      "tickets 0.3513776064778623\n",
      "ten 0.3518745150982593\n",
      "dist to year: 0.3839689926240508\n",
      "testing: week - day = hour - minute\n",
      "got: week - day = seconds - minute\n",
      "closest 10:\n",
      "minute 0.10719770198326772\n",
      "seconds 0.2876826990648309\n",
      "minutes 0.3187758889692133\n",
      "extra 0.3438550123114934\n",
      "slot 0.3439673648278846\n",
      "hour 0.3522579091056406\n",
      "switch 0.3641705639380399\n",
      "drivers 0.3691617662429443\n",
      "week 0.37587600200431714\n",
      "onboard 0.3806735866652722\n",
      "dist to hour: 0.3522579091056406\n",
      "testing: france - paris = italy - rome\n",
      "got: france - paris = emperor - rome\n",
      "closest 10:\n",
      "emperor 0.17532023212858738\n",
      "constantinople 0.1890764595112655\n",
      "papal 0.19176631657939724\n",
      "roman 0.19309906718618453\n",
      "frankish 0.19353657029828342\n",
      "rome 0.19848508532252573\n",
      "rulers 0.19981331769255528\n",
      "holy 0.207142617653446\n",
      "imperial 0.20856378053632718\n",
      "reign 0.2113010836511089\n",
      "dist to italy: 0.2440527466035488\n",
      "testing: paris - france = rome - italy\n",
      "got: paris - france = milan - italy\n",
      "closest 10:\n",
      "paris 0.10672985090849363\n",
      "milan 0.20771531198309678\n",
      "venice 0.24321782731163288\n",
      "leipzig 0.2493827237232209\n",
      "dresden 0.2654287081406359\n",
      "vienna 0.2709996350139371\n",
      "giovanni 0.27118089660080436\n",
      "bologna 0.28216956789069925\n",
      "manet 0.290223699191954\n",
      "choir 0.3112638166406346\n",
      "dist to rome: 0.33036941512157647\n",
      "testing: france - french = england - english\n",
      "got: france - french = england - english\n",
      "closest 10:\n",
      "england 0.15910922132219163\n",
      "english 0.21674623668400317\n",
      "ireland 0.24082612867623598\n",
      "scottish 0.25977828504651357\n",
      "scotland 0.27859063905300396\n",
      "kingdom 0.28356679109525773\n",
      "netherlands 0.2886928136174012\n",
      "essex 0.2918674633155858\n",
      "france 0.3021325960995479\n",
      "britain 0.30493549276151377\n",
      "dist to england: 0.15910922132219185\n",
      "testing: japan - japanese = china - chinese\n",
      "got: japan - japanese = china - chinese\n",
      "closest 10:\n",
      "china 0.15491584153096694\n",
      "chinese 0.19711197299415073\n",
      "asia 0.27272872154130445\n",
      "cuisines 0.30488757972182523\n",
      "confucianism 0.3240352524357847\n",
      "asian 0.3371481957703387\n",
      "cuisine 0.33974946661421124\n",
      "chinas 0.35626701324495\n",
      "peoples 0.3567565064480197\n",
      "japan 0.3573010540982754\n",
      "dist to china: 0.15491584153096705\n",
      "testing: china - chinese = america - american\n",
      "got: china - chinese = united - american\n",
      "closest 10:\n",
      "united 0.15592429858392776\n",
      "american 0.17795269509886602\n",
      "america 0.19836393398537777\n",
      "states 0.22493036554394996\n",
      "establishments 0.2406903487448646\n",
      "los 0.27248522351069127\n",
      "1970 0.2775571178313829\n",
      "1992 0.27769094369728065\n",
      "1989 0.31340976760579486\n",
      "mexico 0.314195237518615\n",
      "dist to america: 0.19836393398537777\n",
      "testing: japan - japanese = italy - italian\n",
      "got: japan - japanese = italy - italian\n",
      "closest 10:\n",
      "italian 0.18436227411172756\n",
      "italy 0.21621413081265328\n",
      "hungarian 0.25469136460119013\n",
      "disestablishments 0.2620647878467708\n",
      "hungary 0.27543802841750686\n",
      "romanian 0.2787566607642814\n",
      "france 0.318417087854722\n",
      "painters 0.32640965214246354\n",
      "austrian 0.329425456219716\n",
      "bulgarian 0.33139594693289\n",
      "dist to italy: 0.21621413081265306\n",
      "testing: japan - japanese = australia - australian\n",
      "got: japan - japanese = australia - australian\n",
      "closest 10:\n",
      "australia 0.18627933712056077\n",
      "australian 0.18671351521976232\n",
      "commonwealth 0.24992307804983893\n",
      "nations 0.2709251057611185\n",
      "member 0.27163060048030585\n",
      "european 0.2940425702424204\n",
      "referendum 0.311685182536966\n",
      "zealand 0.31287512686366037\n",
      "luxembourg 0.3169041922985667\n",
      "signed 0.31842574959242986\n",
      "dist to australia: 0.18627933712056088\n",
      "testing: walk - walking = swim - swimming\n",
      "got: walk - walking = hosts - swimming\n",
      "closest 10:\n",
      "swimming 0.15809965329530495\n",
      "hosts 0.31480405520170585\n",
      "athletic 0.36356582758212297\n",
      "basketball 0.37439598904151405\n",
      "sports 0.38924559286768357\n",
      "clubs 0.3919235975019024\n",
      "trail 0.39873938904857464\n",
      "hockey 0.40441300232768107\n",
      "compete 0.4123118659850532\n",
      "mens 0.4130234432611425\n",
      "dist to swim: 0.6286296285847024\n",
      "**********\n",
      "testing: king - man = queen - woman\n",
      "got: king - man = throne - woman\n",
      "closest 10:\n",
      "king 0.17770749586447032\n",
      "throne 0.18532640468446515\n",
      "heir 0.20418457616929486\n",
      "sons 0.2260403746542493\n",
      "queen 0.23812642027440734\n",
      "regent 0.2609757364147266\n",
      "princess 0.27141443075503546\n",
      "son 0.28591337418229346\n",
      "empress 0.28745414893920407\n",
      "duke 0.28751515100640346\n",
      "dist to queen: 0.23812642027440722\n",
      "testing: king - prince = queen - princess\n",
      "got: king - prince = legend - princess\n",
      "closest 10:\n",
      "princess 0.10315394602795791\n",
      "king 0.1884470692780682\n",
      "legend 0.2757080233668546\n",
      "uncle 0.28920538311089017\n",
      "son 0.324112475419271\n",
      "feast 0.3277072108353025\n",
      "dragon 0.3283135307056634\n",
      "tale 0.34535574319275053\n",
      "her 0.3472673494477281\n",
      "throne 0.3589005415513995\n",
      "dist to queen: 0.39477210825270437\n",
      "testing: miami - florida = dallas - texas\n",
      "got: miami - florida = footballer - texas\n",
      "closest 10:\n",
      "miami 0.14085831473716326\n",
      "texas 0.285951664300204\n",
      "footballer 0.3494291316875251\n",
      "jackson 0.35574354977169764\n",
      "joe 0.3575424802812587\n",
      "orleans 0.3581431401394144\n",
      "longtime 0.3716113968425574\n",
      "harry 0.3722256334266316\n",
      "mascot 0.3889396251445505\n",
      "atlanta 0.39459954568048994\n",
      "dist to dallas: 0.5065512664726958\n",
      "testing: einstein - scientist = picasso - painter\n",
      "got: einstein - scientist = friedrich - painter\n",
      "closest 10:\n",
      "painter 0.26718900470978735\n",
      "einstein 0.33344939012444585\n",
      "friedrich 0.361089647520629\n",
      "1874 0.3677615676088436\n",
      "claude 0.3862636178616635\n",
      "manet 0.4014470584190435\n",
      "schrödinger 0.4024734056074669\n",
      "giovanni 0.4033682001279555\n",
      "mathematician 0.4044640893301774\n",
      "gauss 0.41159286965702546\n",
      "dist to picasso: 0.6076901294002541\n",
      "testing: japan - sushi = germany - bratwurst\n",
      "Sorry, sushi not in word2idx\n",
      "testing: man - woman = he - she\n",
      "got: man - woman = biographer - she\n",
      "closest 10:\n",
      "biographer 0.20036079516874272\n",
      "him 0.21424788082061919\n",
      "my 0.2187489160014946\n",
      "told 0.2236443765413073\n",
      "he 0.22366892968873997\n",
      "friend 0.2281764101508016\n",
      "i 0.23278399696077623\n",
      "she 0.23341576089228622\n",
      "friends 0.24676331761115766\n",
      "man 0.2602683384395147\n",
      "dist to he: 0.22366892968873986\n",
      "testing: man - woman = uncle - aunt\n",
      "got: man - woman = friend - aunt\n",
      "closest 10:\n",
      "aunt 0.11127225130222529\n",
      "friend 0.2967380594768487\n",
      "orwell 0.314578374878029\n",
      "biographer 0.3152451089221664\n",
      "brother 0.3207839486087388\n",
      "him 0.3280646518590393\n",
      "he 0.3294938395818221\n",
      "nephew 0.33899285438890725\n",
      "friends 0.3539465572667734\n",
      "father 0.35403140145404643\n",
      "dist to uncle: 0.3767103838134739\n",
      "testing: man - woman = brother - sister\n",
      "got: man - woman = brother - sister\n",
      "closest 10:\n",
      "sister 0.21322795439340925\n",
      "brother 0.2804698628365353\n",
      "daughter 0.29728924290442316\n",
      "uncle 0.32984432178442846\n",
      "nephew 0.33658900571473427\n",
      "victor 0.3430192665585873\n",
      "father 0.3439223519516401\n",
      "granddaughter 0.3493024817650604\n",
      "son 0.36444543195044876\n",
      "he 0.3726989026188565\n",
      "dist to brother: 0.2804698628365354\n",
      "testing: man - woman = husband - wife\n",
      "got: man - woman = him - wife\n",
      "closest 10:\n",
      "wife 0.1805171017163374\n",
      "him 0.19015565274318635\n",
      "he 0.19366301018454668\n",
      "son 0.20026962288478078\n",
      "brother 0.21375855276947364\n",
      "father 0.2247565264520558\n",
      "biographer 0.22650650665301963\n",
      "nephew 0.22691759347160312\n",
      "himself 0.24038202892548022\n",
      "friend 0.24568866863021155\n",
      "dist to husband: 0.43303230534158266\n",
      "testing: man - woman = actor - actress\n",
      "got: man - woman = starring - actress\n",
      "closest 10:\n",
      "actress 0.12012971656112803\n",
      "starring 0.19697933859686545\n",
      "starred 0.2041839157768771\n",
      "dave 0.21947436688247135\n",
      "actor 0.22847015935884285\n",
      "kaufman 0.23020950499270254\n",
      "screenwriter 0.24633221266237282\n",
      "ian 0.2533645667548782\n",
      "tom 0.2560324694467767\n",
      "danny 0.2560501036667141\n",
      "dist to actor: 0.22847015935884285\n",
      "testing: man - woman = father - mother\n",
      "got: man - woman = father - mother\n",
      "closest 10:\n",
      "father 0.20077439618998816\n",
      "mother 0.22355850421465062\n",
      "brother 0.23158936396023933\n",
      "son 0.26282431143074847\n",
      "him 0.28632084805520164\n",
      "his 0.29458197014633913\n",
      "himself 0.2956883102563992\n",
      "uncle 0.30125582303215925\n",
      "he 0.3013444442643889\n",
      "wife 0.3084424161449083\n",
      "dist to father: 0.20077439618998816\n",
      "testing: heir - heiress = prince - princess\n",
      "Sorry, heiress not in word2idx\n",
      "testing: nephew - niece = uncle - aunt\n",
      "got: nephew - niece = marie - aunt\n",
      "closest 10:\n",
      "aunt 0.24881592185078905\n",
      "nephew 0.2527941387025202\n",
      "marie 0.3258599758956999\n",
      "wife 0.33232951466899285\n",
      "brother 0.3377809071832244\n",
      "son 0.35140576932893774\n",
      "georges 0.3620836574228552\n",
      "father 0.3643341311105346\n",
      "soon 0.37224720731783156\n",
      "died 0.37771805632664046\n",
      "dist to uncle: 0.45548303706930726\n",
      "testing: france - paris = japan - tokyo\n",
      "got: france - paris = japan - tokyo\n",
      "closest 10:\n",
      "tokyo 0.20344904333123026\n",
      "japan 0.23199149926139961\n",
      "japanese 0.3360257004337919\n",
      "japans 0.36328691261147295\n",
      "zealand 0.3730234214046584\n",
      "vietnam 0.3823735474446117\n",
      "kong 0.4055391014221259\n",
      "australia 0.41052575757083776\n",
      "tanaka 0.41413432249605575\n",
      "launch 0.42050581047877156\n",
      "dist to japan: 0.23199149926139973\n",
      "testing: france - paris = china - beijing\n",
      "got: france - paris = china - beijing\n",
      "closest 10:\n",
      "beijing 0.11850836860815994\n",
      "china 0.2573197001133951\n",
      "korea 0.26512711302734016\n",
      "vietnamese 0.2779003477894023\n",
      "yuan 0.30727931037450784\n",
      "taiwan 0.3090822081050314\n",
      "republics 0.323461752020638\n",
      "qing 0.3388556443614541\n",
      "korean 0.34492306134827755\n",
      "asia 0.35085327495321905\n",
      "dist to china: 0.2573197001133952\n",
      "testing: february - january = december - november\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got: february - january = april - november\n",
      "closest 10:\n",
      "november 0.04389081055620636\n",
      "february 0.06443969753050205\n",
      "april 0.0842505033730847\n",
      "june 0.09428211812767562\n",
      "july 0.0973623083526638\n",
      "march 0.0991023084419611\n",
      "october 0.10015041236284139\n",
      "august 0.10588506187036062\n",
      "september 0.11054167467668963\n",
      "december 0.11566971245410951\n",
      "dist to december: 0.11566971245410962\n",
      "testing: france - paris = germany - berlin\n",
      "got: france - paris = germany - berlin\n",
      "closest 10:\n",
      "germany 0.15234671448898807\n",
      "poland 0.20462105508791217\n",
      "germanys 0.22046075807968601\n",
      "berlin 0.22801812430557178\n",
      "russia 0.23100404559346344\n",
      "austria 0.24117948952300527\n",
      "bloc 0.2642687443583738\n",
      "prussia 0.2821408443822323\n",
      "hungary 0.28798618837436274\n",
      "republics 0.2997489253127186\n",
      "dist to germany: 0.15234671448898807\n",
      "testing: week - day = year - month\n",
      "got: week - day = months - month\n",
      "closest 10:\n",
      "week 0.14179054237860245\n",
      "month 0.15479119476822278\n",
      "months 0.2701211385969464\n",
      "year 0.29701302617441117\n",
      "days 0.3086557502538636\n",
      "weeks 0.31448380179461366\n",
      "14 0.3208201254168336\n",
      "12 0.3424285134987195\n",
      "ten 0.34703346198195206\n",
      "10 0.35746875134420375\n",
      "dist to year: 0.2970130261744113\n",
      "testing: week - day = hour - minute\n",
      "got: week - day = minutes - minute\n",
      "closest 10:\n",
      "minute 0.11674658747538014\n",
      "minutes 0.2723643409059785\n",
      "seconds 0.29293628908011504\n",
      "extra 0.3055271406846958\n",
      "slot 0.3278211822493008\n",
      "hour 0.3381454812515764\n",
      "week 0.35448468969799796\n",
      "hours 0.3836771461554912\n",
      "10 0.3864609974750197\n",
      "switch 0.3871213033071047\n",
      "dist to hour: 0.3381454812515763\n",
      "testing: france - paris = italy - rome\n",
      "got: france - paris = emperor - rome\n",
      "closest 10:\n",
      "emperor 0.20980509737703046\n",
      "frankish 0.22411320013722313\n",
      "roman 0.22532410882490017\n",
      "rome 0.22885219088759734\n",
      "constantinople 0.2332003110843277\n",
      "papal 0.2365871119396018\n",
      "rulers 0.24384938010529056\n",
      "imperial 0.24994639395527607\n",
      "francia 0.2536144038257899\n",
      "supremacy 0.2584512580942083\n",
      "dist to italy: 0.3023312887536449\n",
      "testing: paris - france = rome - italy\n",
      "got: paris - france = milan - italy\n",
      "closest 10:\n",
      "paris 0.12914978079483364\n",
      "milan 0.21849663262635688\n",
      "venice 0.26367727681339304\n",
      "dresden 0.2829983585006467\n",
      "vienna 0.2853354366198686\n",
      "bologna 0.2922773712662481\n",
      "leipzig 0.3003240765163757\n",
      "palace 0.31689065867635346\n",
      "giovanni 0.32362583839508285\n",
      "di 0.3441214902720907\n",
      "dist to rome: 0.3857363987165715\n",
      "testing: france - french = england - english\n",
      "got: france - french = england - english\n",
      "closest 10:\n",
      "england 0.1966288747301791\n",
      "english 0.23133570021461047\n",
      "ireland 0.29792125427778804\n",
      "scottish 0.3250454637902571\n",
      "dutch 0.35309262103265493\n",
      "netherlands 0.35541116656314364\n",
      "kingdom 0.3600404991846573\n",
      "scotland 0.37535319638816955\n",
      "irish 0.37721264875190463\n",
      "gaelic 0.38446714126186443\n",
      "dist to england: 0.19662887473017887\n",
      "testing: japan - japanese = china - chinese\n",
      "got: japan - japanese = china - chinese\n",
      "closest 10:\n",
      "china 0.18409729841556577\n",
      "chinese 0.23320301939228472\n",
      "asia 0.31619532482416635\n",
      "korea 0.35342441972589933\n",
      "cuisines 0.3699314722217433\n",
      "chinas 0.37778858354348854\n",
      "jiang 0.3897425618513213\n",
      "diaspora 0.390316821174445\n",
      "asian 0.39050215373003117\n",
      "han 0.3925531503931967\n",
      "dist to china: 0.18409729841556577\n",
      "testing: china - chinese = america - american\n",
      "got: china - chinese = united - american\n",
      "closest 10:\n",
      "united 0.19959598307915816\n",
      "american 0.21960093145174808\n",
      "america 0.23983569687219009\n",
      "states 0.290819499669811\n",
      "1970 0.30028114662897354\n",
      "mexico 0.30751570365810554\n",
      "establishments 0.3339149179739559\n",
      "producers 0.33990080077824436\n",
      "los 0.34225519817651684\n",
      "california 0.35688529181991335\n",
      "dist to america: 0.23983569687219042\n",
      "testing: japan - japanese = italy - italian\n",
      "got: japan - japanese = italy - italian\n",
      "closest 10:\n",
      "italian 0.22733493866824772\n",
      "italy 0.2373351815557292\n",
      "hungarian 0.26794306522932876\n",
      "hungary 0.27578941460917417\n",
      "disestablishments 0.31832417317340034\n",
      "romanian 0.3216000362873941\n",
      "bulgarian 0.3479807803599664\n",
      "austria 0.3604118387342865\n",
      "austrian 0.36447062247252293\n",
      "1806 0.36648740392205914\n",
      "dist to italy: 0.23733518155572897\n",
      "testing: japan - japanese = australia - australian\n",
      "got: japan - japanese = australia - australian\n",
      "closest 10:\n",
      "australia 0.1894478498223392\n",
      "australian 0.21667573688475328\n",
      "commonwealth 0.28742887583574994\n",
      "zealand 0.29195976675623914\n",
      "member 0.33177137116453737\n",
      "referendum 0.3416045094991179\n",
      "uk 0.3496342524899998\n",
      "european 0.35187481560361045\n",
      "nations 0.3594063902382971\n",
      "luxembourg 0.3663874810302957\n",
      "dist to australia: 0.1894478498223393\n",
      "testing: walk - walking = swim - swimming\n",
      "got: walk - walking = pools - swimming\n",
      "closest 10:\n",
      "swimming 0.15198654756575203\n",
      "pools 0.3567883661533173\n",
      "athletic 0.3866497477873384\n",
      "2–3 0.38696036230914\n",
      "hosts 0.38831631974259884\n",
      "basketball 0.39707623790616653\n",
      "sports 0.40966794811833473\n",
      "compete 0.4132034128819454\n",
      "indoor 0.4190476811781142\n",
      "stadium 0.42058816999427073\n",
      "dist to swim: 0.5733024434786487\n"
     ]
    }
   ],
   "source": [
    "def get_context(pos, sentence, window_size):\n",
    "  # input:\n",
    "  # a sentence of the form: x x x x c c c pos c c c x x x x\n",
    "  # output:\n",
    "  # the context word indices: c c c c c c\n",
    "\n",
    "  start = max(0, pos - window_size)\n",
    "  end_  = min(len(sentence), pos + window_size)\n",
    "\n",
    "  context = []\n",
    "  for ctx_pos, ctx_word_idx in enumerate(sentence[start:end_], start=start):\n",
    "    if ctx_pos != pos:\n",
    "      # don't include the input word itself as a target\n",
    "      context.append(ctx_word_idx)\n",
    "  return context\n",
    "\n",
    "\n",
    "def sgd(input_, targets, label, learning_rate, W, V):\n",
    "  # W[input_] shape: D\n",
    "  # V[:,targets] shape: D x N\n",
    "  # activation shape: N\n",
    "  # print(\"input_:\", input_, \"targets:\", targets)\n",
    "  activation = W[input_].dot(V[:,targets])\n",
    "  prob = sigmoid(activation)\n",
    "\n",
    "  # gradients\n",
    "  gV = np.outer(W[input_], prob - label) # D x N\n",
    "  gW = np.sum((prob - label)*V[:,targets], axis=1) # D\n",
    "\n",
    "  V[:,targets] -= learning_rate*gV # D x N\n",
    "  W[input_] -= learning_rate*gW # D\n",
    "\n",
    "  # return cost (binary cross entropy)\n",
    "  cost = label * np.log(prob + 1e-10) + (1 - label) * np.log(1 - prob + 1e-10)\n",
    "  return cost.sum()\n",
    "\n",
    "\n",
    "def load_model(savedir):\n",
    "  with open('%s/word2idx.json' % savedir) as f:\n",
    "    word2idx = json.load(f)\n",
    "  npz = np.load('%s/weights.npz' % savedir)\n",
    "  W = npz['arr_0']\n",
    "  V = npz['arr_1']\n",
    "  return word2idx, W, V\n",
    "\n",
    "\n",
    "\n",
    "def analogy(pos1, neg1, pos2, neg2, word2idx, idx2word, W):\n",
    "    V, D = W.shape\n",
    "\n",
    "    # don't actually use pos2 in calculation, just print what's expected\n",
    "    print(\"testing: %s - %s = %s - %s\" % (pos1, neg1, pos2, neg2))\n",
    "    for w in (pos1, neg1, pos2, neg2):\n",
    "        if w not in word2idx:\n",
    "          print(\"Sorry, %s not in word2idx\" % w)\n",
    "          return\n",
    "\n",
    "    p1 = W[word2idx[pos1]]\n",
    "    n1 = W[word2idx[neg1]]\n",
    "    p2 = W[word2idx[pos2]]\n",
    "    n2 = W[word2idx[neg2]]\n",
    "\n",
    "    vec = p1 - n1 + n2\n",
    "\n",
    "    distances = pairwise_distances(vec.reshape(1, D), W, metric='cosine').reshape(V)\n",
    "    idx = distances.argsort()[:10]\n",
    "\n",
    "    # pick one that's not p1, n1, or n2\n",
    "    best_idx = -1\n",
    "    keep_out = [word2idx[w] for w in (pos1, neg1, neg2)]\n",
    "    # print(\"keep_out:\", keep_out)\n",
    "    for i in idx:\n",
    "        if i not in keep_out:\n",
    "          best_idx = i\n",
    "          break\n",
    "        # print(\"best_idx:\", best_idx)\n",
    "\n",
    "    print(\"got: %s - %s = %s - %s\" % (pos1, neg1, idx2word[best_idx], neg2))\n",
    "    print(\"closest 10:\")\n",
    "    for i in idx:\n",
    "        print(idx2word[i], distances[i])\n",
    "\n",
    "    print(\"dist to %s:\" % pos2, cos_dist(p2, vec))\n",
    "\n",
    "\n",
    "def test_model(word2idx, W, V):\n",
    "  # there are multiple ways to get the \"final\" word embedding\n",
    "  # We = (W + V.T) / 2\n",
    "  # We = W\n",
    "    idx2word = {i:w for w, i in word2idx.items()}\n",
    "\n",
    "    for We in (W, (W + V.T) / 2):\n",
    "        print(\"**********\")\n",
    "\n",
    "        analogy('king', 'man', 'queen', 'woman', word2idx, idx2word, We)\n",
    "        analogy('king', 'prince', 'queen', 'princess', word2idx, idx2word, We)\n",
    "        analogy('miami', 'florida', 'dallas', 'texas', word2idx, idx2word, We)\n",
    "        analogy('einstein', 'scientist', 'picasso', 'painter', word2idx, idx2word, We)\n",
    "        analogy('japan', 'sushi', 'germany', 'bratwurst', word2idx, idx2word, We)\n",
    "        analogy('man', 'woman', 'he', 'she', word2idx, idx2word, We)\n",
    "        analogy('man', 'woman', 'uncle', 'aunt', word2idx, idx2word, We)\n",
    "        analogy('man', 'woman', 'brother', 'sister', word2idx, idx2word, We)\n",
    "        analogy('man', 'woman', 'husband', 'wife', word2idx, idx2word, We)\n",
    "        analogy('man', 'woman', 'actor', 'actress', word2idx, idx2word, We)\n",
    "        analogy('man', 'woman', 'father', 'mother', word2idx, idx2word, We)\n",
    "        analogy('heir', 'heiress', 'prince', 'princess', word2idx, idx2word, We)\n",
    "        analogy('nephew', 'niece', 'uncle', 'aunt', word2idx, idx2word, We)\n",
    "        analogy('france', 'paris', 'japan', 'tokyo', word2idx, idx2word, We)\n",
    "        analogy('france', 'paris', 'china', 'beijing', word2idx, idx2word, We)\n",
    "        analogy('february', 'january', 'december', 'november', word2idx, idx2word, We)\n",
    "        analogy('france', 'paris', 'germany', 'berlin', word2idx, idx2word, We)\n",
    "        analogy('week', 'day', 'year', 'month', word2idx, idx2word, We)\n",
    "        analogy('week', 'day', 'hour', 'minute', word2idx, idx2word, We)\n",
    "        analogy('france', 'paris', 'italy', 'rome', word2idx, idx2word, We)\n",
    "        analogy('paris', 'france', 'rome', 'italy', word2idx, idx2word, We)\n",
    "        analogy('france', 'french', 'england', 'english', word2idx, idx2word, We)\n",
    "        analogy('japan', 'japanese', 'china', 'chinese', word2idx, idx2word, We)\n",
    "        analogy('china', 'chinese', 'america', 'american', word2idx, idx2word, We)\n",
    "        analogy('japan', 'japanese', 'italy', 'italian', word2idx, idx2word, We)\n",
    "        analogy('japan', 'japanese', 'australia', 'australian', word2idx, idx2word, We)\n",
    "        analogy('walk', 'walking', 'swim', 'swimming', word2idx, idx2word, We)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     word2idx, W, V = train_model('w2v_model')\n",
    "    word2idx, W, V = load_model('w2v_model')\n",
    "    test_model(word2idx, W, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
